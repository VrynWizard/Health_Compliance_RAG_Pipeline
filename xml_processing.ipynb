{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_logical_chunks(xml_content, output_json):\n",
    "    \"\"\"\n",
    "    Extract logical chunks from a hospice XML document including sections, comments, and figures.\n",
    "    Save the extracted data to the provided output JSON file.\n",
    "    Improved to handle nested headers with same SOURCE attribute by analyzing numbering patterns.\n",
    "    \"\"\"\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # Initialize document metadata\n",
    "    document_metadata = {\n",
    "        \"title\": None,\n",
    "        \"agency\": None,\n",
    "        \"subagency\": None,\n",
    "        \"cfr\": None,\n",
    "        \"subject\": None,\n",
    "        \"effective_date\": None,\n",
    "        \"contact_info\": [],\n",
    "        \"summary\": None\n",
    "    }\n",
    "\n",
    "    # Extract preamble information (if present)\n",
    "    preamble = root.find(\".//PREAMB\")\n",
    "    if preamble is not None:\n",
    "        agency_elem = preamble.find(\"./AGENCY\")\n",
    "        if agency_elem is not None:\n",
    "            document_metadata[\"agency\"] = \"\".join(agency_elem.itertext()).strip()\n",
    "\n",
    "        subagency_elem = preamble.find(\"./SUBAGY\")\n",
    "        if subagency_elem is not None:\n",
    "            document_metadata[\"subagency\"] = \"\".join(subagency_elem.itertext()).strip()\n",
    "\n",
    "        cfr_elem = preamble.find(\"./CFR\")\n",
    "        if cfr_elem is not None:\n",
    "            document_metadata[\"cfr\"] = \"\".join(cfr_elem.itertext()).strip()\n",
    "\n",
    "        subject_elem = preamble.find(\"./SUBJECT\")\n",
    "        if subject_elem is not None:\n",
    "            document_metadata[\"title\"] = \"\".join(subject_elem.itertext()).strip()\n",
    "            document_metadata[\"subject\"] = \"\".join(subject_elem.itertext()).strip()\n",
    "\n",
    "        effdate_elem = preamble.find(\"./EFFDATE/P\")\n",
    "        if effdate_elem is not None:\n",
    "            document_metadata[\"effective_date\"] = \"\".join(effdate_elem.itertext()).strip()\n",
    "\n",
    "        for contact in preamble.findall(\"./FURINF/P\"):\n",
    "            contact_text = \"\".join(contact.itertext()).strip()\n",
    "            if contact_text:\n",
    "                document_metadata[\"contact_info\"].append(contact_text)\n",
    "\n",
    "        summary_elem = preamble.find(\"./SUM/P\")\n",
    "        if summary_elem is not None:\n",
    "            document_metadata[\"summary\"] = \"\".join(summary_elem.itertext()).strip()\n",
    "\n",
    "    # Process additional sections and figures\n",
    "    extracted_data = []\n",
    "    figures = []\n",
    "    current_page = None\n",
    "    current_comment = current_response = current_final_decision = None\n",
    "\n",
    "    # Define regex patterns for different header formats\n",
    "    number_patterns = {\n",
    "        \"roman_upper\": re.compile(r\"^([IVXLCDM]+)\\.\\s\"),          # I., II., etc.\n",
    "        \"roman_lower\": re.compile(r\"^([ivxlcdm]+)\\.\\s\"),          # i., ii., etc.\n",
    "        \"numeric\": re.compile(r\"^(\\d+)\\.\\s\"),                     # 1., 2., etc.\n",
    "        \"alpha_lower\": re.compile(r\"^([a-z])\\.\\s\"),               # a., b., etc.\n",
    "        \"alpha_upper\": re.compile(r\"^([A-Z])\\.\\s\"),               # A., B., etc.\n",
    "        \"numeric_paren\": re.compile(r\"^\\((\\d+)\\)\\s\"),             # (1), (2), etc.\n",
    "        \"alpha_lower_paren\": re.compile(r\"^\\(([a-z])\\)\\s\"),       # (a), (b), etc.\n",
    "        \"alpha_upper_paren\": re.compile(r\"^\\(([A-Z])\\)\\s\")        # (A), (B), etc.\n",
    "    }\n",
    "\n",
    "    # Track the hierarchical structure\n",
    "    current_headers = {\n",
    "        \"level_1\": None,\n",
    "        \"level_2\": None,\n",
    "        \"level_3\": None,\n",
    "        \"level_4\": None,\n",
    "        \"level_5\": None\n",
    "    }\n",
    "\n",
    "    # Keep track of the format used at each level\n",
    "    level_formats = {\n",
    "        \"level_1\": None,\n",
    "        \"level_2\": None,\n",
    "        \"level_3\": None,\n",
    "        \"level_4\": None,\n",
    "        \"level_5\": None\n",
    "    }\n",
    "\n",
    "    def get_header_pattern(header_text):\n",
    "        \"\"\"Identify the pattern type of a header based on its formatting\"\"\"\n",
    "        for pattern_name, pattern in number_patterns.items():\n",
    "            match = pattern.match(header_text)\n",
    "            if match:\n",
    "                return pattern_name, match.group(1)\n",
    "        return None, None\n",
    "\n",
    "    def determine_header_level(header_text, source_type):\n",
    "        \"\"\"\n",
    "        Determine the appropriate level for a header based on both its SOURCE attribute\n",
    "        and its numbering pattern\n",
    "        \"\"\"\n",
    "        pattern_type, pattern_value = get_header_pattern(header_text)\n",
    "\n",
    "        # Base level from SOURCE attribute\n",
    "        if source_type == \"HD1\":\n",
    "            base_level = 1\n",
    "        elif source_type == \"HD2\":\n",
    "            base_level = 2\n",
    "        elif source_type == \"HD3\":\n",
    "            base_level = 3\n",
    "        else:\n",
    "            base_level = 3  # Default for other source types\n",
    "\n",
    "        # If there's no pattern, we use the SOURCE level\n",
    "        if not pattern_type:\n",
    "            return base_level, None, None\n",
    "\n",
    "        # Check if this pattern is already used at a specific level\n",
    "        for level, format_type in level_formats.items():\n",
    "            if format_type == pattern_type:\n",
    "                level_num = int(level.split('_')[1])\n",
    "                return level_num, pattern_type, pattern_value\n",
    "\n",
    "        # If we find a new pattern, assign it to the first empty level\n",
    "        # starting from base_level\n",
    "        for i in range(base_level, 6):\n",
    "            level_key = f\"level_{i}\"\n",
    "            if level_formats[level_key] is None:\n",
    "                return i, pattern_type, pattern_value\n",
    "\n",
    "        # Fallback\n",
    "        return base_level, pattern_type, pattern_value\n",
    "\n",
    "    for elem in root.iter():\n",
    "        if elem.tag == \"PRTPAGE\":\n",
    "            current_page = elem.attrib.get(\"P\", None)\n",
    "\n",
    "        if elem.tag == \"GPH\":\n",
    "            figure_id = elem.find(\"GID\").text if elem.find(\"GID\") is not None else \"Unknown\"\n",
    "            figure_span = elem.attrib.get(\"SPAN\", \"1\")\n",
    "            figure_deep = elem.attrib.get(\"DEEP\", \"0\")\n",
    "            figures.append({\n",
    "                \"figure_id\": figure_id,\n",
    "                \"page_number\": current_page,\n",
    "                \"span\": figure_span,\n",
    "                \"deep\": figure_deep\n",
    "            })\n",
    "\n",
    "        if elem.tag == \"HD\":\n",
    "            header_text = \"\".join(elem.itertext()).strip()\n",
    "            header_type = elem.attrib.get(\"SOURCE\", \"\")\n",
    "\n",
    "            # Determine the appropriate level for this header\n",
    "            level, pattern_type, pattern_value = determine_header_level(header_text, header_type)\n",
    "\n",
    "            # If we found a pattern, update the format used at this level\n",
    "            if pattern_type and level_formats[f\"level_{level}\"] is None:\n",
    "                level_formats[f\"level_{level}\"] = pattern_type\n",
    "\n",
    "            # Update the current header at this level\n",
    "            current_headers[f\"level_{level}\"] = header_text\n",
    "\n",
    "            # Clear any lower-level headers\n",
    "            for i in range(level + 1, 6):\n",
    "                current_headers[f\"level_{i}\"] = None\n",
    "\n",
    "            # Build the section path\n",
    "            section_path = []\n",
    "            for i in range(1, 6):\n",
    "                header = current_headers[f\"level_{i}\"]\n",
    "                if header:\n",
    "                    section_path.append(header)\n",
    "\n",
    "            section_name = \" > \".join(section_path)\n",
    "\n",
    "            # Create section entry\n",
    "            section_entry = {\n",
    "                \"section_name\": section_name,\n",
    "                \"section_level_1\": current_headers[\"level_1\"],\n",
    "                \"section_level_2\": current_headers[\"level_2\"],\n",
    "                \"section_level_3\": current_headers[\"level_3\"],\n",
    "                \"section_level_4\": current_headers[\"level_4\"],\n",
    "                \"section_level_5\": current_headers[\"level_5\"],\n",
    "                \"text_content\": \"\",\n",
    "                \"page_number\": current_page,\n",
    "                \"comments_responses\": [],\n",
    "                \"final_decision\": None\n",
    "            }\n",
    "\n",
    "            extracted_data.append(section_entry)\n",
    "\n",
    "        if elem.tag == \"P\":\n",
    "            para_text = \"\".join(elem.itertext()).strip()\n",
    "            if not para_text:\n",
    "                continue\n",
    "\n",
    "            # Handle comments, responses, and final decisions\n",
    "            if para_text.startswith(\"Comment:\") or any(e is not None and e.text == \"Comment:\" for e in elem.findall(\".//E\")):\n",
    "                current_comment = para_text\n",
    "                current_response = current_final_decision = None\n",
    "            elif para_text.startswith(\"Response:\") or any(e is not None and e.text == \"Response:\" for e in elem.findall(\".//E\")):\n",
    "                current_response = para_text\n",
    "                if current_comment and extracted_data:\n",
    "                    extracted_data[-1][\"comments_responses\"].append({\n",
    "                        \"comment\": current_comment,\n",
    "                        \"response\": current_response\n",
    "                    })\n",
    "                current_final_decision = None\n",
    "            elif para_text.startswith(\"Final Decision:\") or any(e is not None and e.text == \"Final Decision:\" for e in elem.findall(\".//E\")):\n",
    "                current_final_decision = para_text\n",
    "                if extracted_data:\n",
    "                    extracted_data[-1][\"final_decision\"] = current_final_decision\n",
    "                current_comment = current_response = None\n",
    "            else:\n",
    "                # Regular paragraph text\n",
    "                if extracted_data:\n",
    "                    if extracted_data[-1][\"text_content\"]:\n",
    "                        extracted_data[-1][\"text_content\"] += \"\\n\\n\" + para_text\n",
    "                    else:\n",
    "                        extracted_data[-1][\"text_content\"] = para_text\n",
    "\n",
    "    # Filter out sections with both empty text_content and empty comments_responses\n",
    "    filtered_data = [\n",
    "        section for section in extracted_data\n",
    "        if section[\"text_content\"].strip() or section[\"comments_responses\"] or section[\"final_decision\"]\n",
    "    ]\n",
    "\n",
    "    # Save the filtered data\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"metadata\": document_metadata,\n",
    "            \"sections\": filtered_data,\n",
    "            \"figures\": figures\n",
    "        }, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Logical chunking completed. Data saved to {output_json}\")\n",
    "    print(f\"Filtered out {len(extracted_data) - len(filtered_data)} empty sections.\")\n",
    "\n",
    "def process_file(xml_file_path, output_json):\n",
    "    with open(xml_file_path, 'r', encoding='utf-8') as f:\n",
    "        xml_content = f.read()\n",
    "    extract_logical_chunks(xml_content, output_json)\n",
    "\n",
    "def process_all_files(xml_folder, json_folder):\n",
    "    if not os.path.exists(json_folder):\n",
    "        os.makedirs(json_folder)\n",
    "\n",
    "    for filename in os.listdir(xml_folder):\n",
    "        if filename.endswith(\".xml\"):\n",
    "            xml_path = os.path.join(xml_folder, filename)\n",
    "            json_filename = os.path.splitext(filename)[0] + \".json\"\n",
    "            json_path = os.path.join(json_folder, json_filename)\n",
    "\n",
    "            print(f\"Processing {filename}...\")\n",
    "            process_file(xml_path, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical chunking completed. Data saved to hospice_2025_final_output.json\n",
      "Filtered out 17 empty sections.\n"
     ]
    }
   ],
   "source": [
    "process_file('hospice_2025_final.xml','hospice_2025_final_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical chunking completed. Data saved to snf_2025_final_output.json\n",
      "Filtered out 33 empty sections.\n"
     ]
    }
   ],
   "source": [
    "process_file('snf_2025_final.xml','snf_2025_final_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing hospice_2025_final.xml...\n",
      "Logical chunking completed. Data saved to regulation_files/json_files/hospice_2025_final.json\n",
      "Filtered out 17 empty sections.\n",
      "Processing snf_2025_final.xml...\n",
      "Logical chunking completed. Data saved to regulation_files/json_files/snf_2025_final.json\n",
      "Filtered out 33 empty sections.\n"
     ]
    }
   ],
   "source": [
    "process_all_files(\"regulation_files/xml_files\", \"regulation_files/json_files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
